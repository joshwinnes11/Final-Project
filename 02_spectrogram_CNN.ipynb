{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "import keras\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPool2D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to each directory\n",
    "train_path_real = '/Users/joshwinnes/Library/Mobile Documents/com~apple~CloudDocs/life things/data science/data/for-2seconds/training/real/'\n",
    "train_path_fake = '/Users/joshwinnes/Library/Mobile Documents/com~apple~CloudDocs/life things/data science/data/for-2seconds/training/fake/'\n",
    "\n",
    "test_path_real = '/Users/joshwinnes/Library/Mobile Documents/com~apple~CloudDocs/life things/data science/data/for-2seconds/testing/real/'\n",
    "test_path_fake = '/Users/joshwinnes/Library/Mobile Documents/com~apple~CloudDocs/life things/data science/data/for-2seconds/testing/fake/'\n",
    "\n",
    "validation_path_real = '/Users/joshwinnes/Library/Mobile Documents/com~apple~CloudDocs/life things/data science/data/for-2seconds/validation/real/'\n",
    "validation_path_fake = '/Users/joshwinnes/Library/Mobile Documents/com~apple~CloudDocs/life things/data science/data/for-2seconds/validation/fake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists of filepaths for each audio file in each directory\n",
    "train_real_audio = [os.path.join(train_path_real, file) for file in os.listdir(train_path_real) if file.endswith('.wav')]\n",
    "train_fake_audio = [os.path.join(train_path_fake, file) for file in os.listdir(train_path_fake) if file.endswith('.wav')]\n",
    "\n",
    "validation_real_audio = [os.path.join(validation_path_real, file) for file in os.listdir(validation_path_real) if file.endswith('.wav')]\n",
    "validation_fake_audio = [os.path.join(validation_path_fake, file) for file in os.listdir(validation_path_fake) if file.endswith('.wav')]\n",
    "\n",
    "test_real_audio = [os.path.join(test_path_real, file) for file in os.listdir(test_path_real) if file.endswith('.wav')]\n",
    "test_fake_audio = [os.path.join(test_path_fake, file) for file in os.listdir(test_path_fake) if file.endswith('.wav')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the lists work\n",
    "random_audio_file = train_real_audio[random.randint(0, len(train_real_audio))]\n",
    "ipd.Audio(random_audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_audio_file = train_fake_audio[random.randint(0, len(train_fake_audio))]\n",
    "ipd.Audio(random_audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_audio_file = test_real_audio[random.randint(0, len(test_real_audio))]\n",
    "ipd.Audio(random_audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_audio_file = test_fake_audio[random.randint(0, len(test_fake_audio))]\n",
    "ipd.Audio(random_audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_audio_file = validation_real_audio[random.randint(0, len(validation_real_audio))]\n",
    "ipd.Audio(random_audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_audio_file = validation_fake_audio[random.randint(0, len(validation_fake_audio))]\n",
    "ipd.Audio(random_audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the wafeforms using Librosa\n",
    "x, sr = librosa.load(random_audio_file)\n",
    "plt.figure(figsize=(20,20))\n",
    "librosa.display.waveshow(x, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert audio file to mel-scale spectrogram\n",
    "\n",
    "def convert_to_melscale_spectrogram(file_path):\n",
    "    x, sr = librosa.load(file_path)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=x, sr=sr)\n",
    "    mel_db_spect = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_db_spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing convert_to_melscale_spectrogram() function with audio file above\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(15,15))    \n",
    "# Loop over the axes and plot a random spectrogram on each\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    random_audio_file = validation_fake_audio[random.randint(0, len(validation_fake_audio))]\n",
    "    test_spec = convert_to_melscale_spectrogram(random_audio_file)  # Get the spectrogram for the current file\n",
    "    \n",
    "    # Display the mel spectrogram on the current axis\n",
    "    librosa.display.specshow(test_spec, x_axis='time', y_axis='hz', ax=ax)\n",
    "    \n",
    "    # Set the title and labels for the current subplot\n",
    "    ax.set_title(f'Mel-Scale Spectrogram {i + 1}: AI Generated')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "# Automatically adjust the layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing AI generated mel-scale spectrogram to real spectrogram\n",
    "fig, axs = plt.subplots(2,2, figsize=(15,15))    \n",
    "# Loop over the axes and plot a random spectrogram on each\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    random_audio_file = validation_real_audio[random.randint(0, len(validation_real_audio))]\n",
    "    test_spec = convert_to_melscale_spectrogram(random_audio_file)  # Get the spectrogram for the current file\n",
    "    \n",
    "    # Display the mel spectrogram on the current axis\n",
    "    librosa.display.specshow(test_spec, x_axis='time', y_axis='hz', ax=ax)\n",
    "    \n",
    "    # Set the title and labels for the current subplot\n",
    "    ax.set_title(f'Mel-Scale Spectrogram {i + 1}: Real Voice')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "# Automatically adjust the layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spectrograms as features to train the model\n",
    "def get_features_and_labels(real_audio_files, fake_audio_files):\n",
    "    spec_arr = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in real_audio_files:\n",
    "        spectrogram = convert_to_melscale_spectrogram(file)\n",
    "        spec_arr.append(spectrogram)\n",
    "        labels.append(0)\n",
    "    for file in fake_audio_files:\n",
    "        spectrogram = convert_to_melscale_spectrogram(file)\n",
    "        spec_arr.append(spectrogram)\n",
    "        labels.append(1)\n",
    "    \n",
    "    return np.array(spec_arr), np.array(labels)\n",
    "\n",
    "train_features, train_labels = get_features_and_labels(train_real_audio, train_fake_audio)\n",
    "validation_features, validation_labels = get_features_and_labels(validation_real_audio, validation_fake_audio)\n",
    "test_features, test_labels = get_features_and_labels(test_real_audio, test_fake_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train features shape: {}\".format(train_features.shape))\n",
    "print(\"test features shape: {}\".format(test_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significantly trimmed VGG model to optimize results\n",
    "\n",
    "trimmed_vgg = Sequential()\n",
    "trimmed_vgg.add(Reshape((128, 87, 1),input_shape=train_features.shape[1:])) #input layer\n",
    "\n",
    "trimmed_vgg.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation='relu')) # Concolutional Layers\n",
    "trimmed_vgg.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation='relu')) # another convolutional layer\n",
    "trimmed_vgg.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) # Pooling layer to reduce dimension of input feature maps\n",
    "\n",
    "trimmed_vgg.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation='relu')) # More convolutional layers\n",
    "trimmed_vgg.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation='relu')) # more convolutional layers\n",
    "trimmed_vgg.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) # another pooling layer to reduce dimension of input feature maps\n",
    "\n",
    "trimmed_vgg.add(Conv2D(filters=256, kernel_size = (3,3), padding='same', activation='relu'))\n",
    "trimmed_vgg.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "trimmed_vgg.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "trimmed_vgg.add(Flatten()) #flattening layer to flatten input feature maps in order to link convolutional layers to fully connected layers\n",
    "trimmed_vgg.add(Dense(units=256,activation=\"relu\")) #fully connected layer\n",
    "trimmed_vgg.add(Dense(units=256,activation=\"relu\")) #fully connected layer\n",
    "trimmed_vgg.add(Dense(1, activation=\"sigmoid\")) #sigmoid function outputs binary output based on probability\n",
    "\n",
    "trimmed_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_vgg.compile(optimizer=keras.optimizers.Adam(),\n",
    "                    loss=keras.losses.binary_crossentropy, \n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_vgg_history = trimmed_vgg.fit(train_features,\n",
    "                                      train_labels,\n",
    "                                      validation_data = [validation_features, validation_labels],\n",
    "                                      batch_size = 32,\n",
    "                                      epochs = 10,\n",
    "                                      steps_per_epoch = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "ax1.plot(trimmed_vgg_history.history[\"accuracy\"])\n",
    "ax1.plot(trimmed_vgg_history.history['val_accuracy'])\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.legend([\"Accuracy\",\"Validation Accuracy\"])\n",
    "\n",
    "ax2.plot(trimmed_vgg_history.history[\"loss\"])\n",
    "ax2.plot(trimmed_vgg_history.history[\"val_loss\"])\n",
    "ax2.set_title(\"Loss\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.legend([\"Loss\",\"Validation Loss\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_vgg_loss, trimmed_vgg_accuracy = trimmed_vgg.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trimmed_vgg.predict(test_features, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = (y_pred>0.005).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_pred=y_pred_binary, y_true=test_labels)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
